---
---
title: "Analysis ON<br>Google Play Store Apps"
author: Snehitha Tadapaneni, Sai Rachana Kandikattu, Amrutha Jayachandradhara, Wilona
  Nguyen, Pramod Krishnachari
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
      position: right
  pdf_document:
    toc: true
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, results = "markdown", message = F)
```


# **Introduction**
<div style="text-align: justify;">
In the fast-paced and competitive world of mobile applications, understanding what makes an application successful is a pressing challenge for developers, businesses, and researchers alike. With Android dominating the mobile operating system landscape—powering over 2.5 billion active devices globally (Brandom, 2019)—the Google Play Store has become the central hub for app distribution and user engagement. Offering millions of applications across diverse categories, the platform serves as an invaluable resource for analyzing user behavior and preferences.

The key to an app's success lies in understanding the intricate relationships between various features, such as app category, ratings, reviews, content suitability, and technical compatibility. Identifying patterns within these factors enables developers to design apps that align with user expectations, thereby increasing downloads, engagement, and retention rates. In this project, we leverage the comprehensive dataset from the Google Play Store to predict app success based on these features.

To address this problem, we have adopted a robust machine learning approach, employing and comparing multiple classification models, including Logistic Regression, k-Nearest Neighbors (kNN), Random Forest, Support Vector Machines (SVM), and Extreme Gradient Boosting (XGBoost). By systematically evaluating the performance of these models, we aim to determine the most effective classifier for predicting app success.

This analysis not only provides actionable insights for app developers and stakeholders but also contributes to the broader understanding of user dynamics and preferences in the app ecosystem. Through this research, we aim to bridge the gap between user expectations and app design, empowering developers to make data-driven decisions that enhance user satisfaction and engagement.

</div>


# **Exploratory Data Analysis (EDA)**
<div style="text-align: justify;">
Dataset Overview: Provide an overview of the dataset, including its source, size, and features.
Data Cleaning and Preparation: Explain any preprocessing steps (e.g., removing missing values, outliers, etc.).
Key Insights: Highlight patterns, distributions, correlations, or anomalies in the dataset (e.g., visualizations like histograms, scatter plots, or correlation matrices).
</div>


# **SMART Question**
<div style="text-align: justify;">
Which are the top 5 app categories, as identified by classification models (logistic regression, SVM, XGBoost, KNN, and random forest), that significantly influenced app success (measured by installs) based on app data from 2010 to 2018, and how accurately can these models predict success trends within this time period?
</div>



# **Model Selection Process**

* Logistic Regression:
-> Why?
-> Advs
-> Explain Parameter Selection

* K-Nearest Neighbour:
-> Why?
-> Advs
-> Explain Parameter Selection

* Random Forest Classifier:
-> Why?
-> Advs
-> Explain Parameter Selection

* XG Boost:
-> Why?
-> Advs
-> Explain Parameter Selection


* Support Vector Machine:
-> Why?
-> Advs
-> Explain Parameter Selection




# **Model Evaluation**
<div style="text-align: justify;">
Performance Metrics: Include accuracy, ROC-AUC, sensitivity, specificity, and any other relevant metrics.
Confusion Matrix: Summarize performance using the confusion matrix.
OOB Error: Discuss the OOB error as an internal validation measure.
Training vs. Testing Accuracy: Analyze and compare performance on different datasets to check for overfitting.
</div>


# **Predictions and Interpretability**
<div style="text-align: justify;">
Prediction Use Cases: Examples of actionable insights from the model (e.g., predicting app install categories, identifying trends).
Feature Importance: Highlight the most significant predictors identified by the model.
</div>


# **Reliability and Limitations**
<div style="text-align: justify;">
Reliability of Results: Discuss how reliable the results are, based on metrics, cross-validation, and OOB error.
*Limitations:*
Bias in the dataset.
Model interpretability limitations.
Potential overfitting.
Mitigation Strategies: Discuss how limitations were addressed (e.g., feature engineering, parameter tuning).
</div>


# **Future Work**
<div style="text-align: justify;">
Improvements: Suggest ways to improve the model, such as:
Incorporating additional data.
Trying advanced algorithms or ensembles.
Refining features or engineering new ones.
Further Analysis: Mention areas where additional analysis could add value (e.g., testing for seasonality, applying domain knowledge).
</div>


# **Conclusion**
<div style="text-align: justify;">
Summary of Findings: Summarize the outcomes and their implications.
Impact: Highlight the potential impact of your analysis and predictions.
</div>


# **References**
<div style="text-align: justify;">
Use APA style to cite all sources.
</div>