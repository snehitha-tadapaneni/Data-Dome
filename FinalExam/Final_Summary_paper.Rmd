---
---
title: "Analysis ON<br>Google Play Store Apps"
author: Snehitha Tadapaneni, Sai Rachana Kandikattu, Amrutha Jayachandradhara, Wilona
  Nguyen, Pramod Krishnachari
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
      position: right
  pdf_document:
    toc: true
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, results = "markdown", message = F)
```


# **Introduction**
<div style="text-align: justify;">
In the fast-paced and competitive world of mobile applications, understanding what makes an application successful is a pressing challenge for developers, businesses, and researchers alike. With Android dominating the mobile operating system landscape—powering over 2.5 billion active devices globally (Brandom, 2019)—the Google Play Store has become the central hub for app distribution and user engagement. Offering millions of applications across diverse categories, the platform serves as an invaluable resource for analyzing user behavior and preferences.

The key to an app's success lies in understanding the intricate relationships between various features, such as app category, ratings, reviews, content suitability, and technical compatibility. Identifying patterns within these factors enables developers to design apps that align with user expectations, thereby increasing downloads, engagement, and retention rates. In this project, we leverage the comprehensive dataset from the Google Play Store to predict app success based on these features.

To address this problem, we have adopted a robust machine learning approach, employing and comparing multiple classification models, including Logistic Regression, k-Nearest Neighbors (kNN), Random Forest, Support Vector Machines (SVM), and Extreme Gradient Boosting (XGBoost). By systematically evaluating the performance of these models, we aim to determine the most effective classifier for predicting app success.

This analysis not only provides actionable insights for app developers and stakeholders but also contributes to the broader understanding of user dynamics and preferences in the app ecosystem. Through this research, we aim to bridge the gap between user expectations and app design, empowering developers to make data-driven decisions that enhance user satisfaction and engagement.

</div>


# **Exploratory Data Analysis (EDA)**

### Dataset Overview
<div style="text-align: justify;">
The dataset used in this study is a popular collection of Google Play Store apps, sourced from Kaggle (faisaljanjua0555, 2023). It contains detailed information about 10,841 apps, organized across 13 variables, with each row representing an individual app. The dataset covers a range
of app attributes, such as ratings, reviews, size, category, installs, price, and content rating, providing valuable insights into the
characteristics that contribute to app popularity and user engagement. This comprehensive data serves as a foundation for analyzing patterns in
user behavior, app performance, and trends within the mobile app ecosystem. 
</div>

### Data Cleaning
<div style="text-align: justify;">
The Google Play Store dataset, as analyzed, contains missing values and inconsistent formats across various columns. To clean and prepare this
dataset for analysis, several steps were taken to address these data quality issues. Here is an overview of how missing records were managed
and further improvements that could be considered:

- Duplicates: Removed 1,181 duplicate apps, reducing the dataset from 10,841 to 9,660 unique apps to eliminate redundancy.
- Price: Cleaned dollar symbols for numeric conversion and removed rows with missing or blank prices.
- Type: Replaced one missing value with "Free" based on a logical inference.
- Size: Standardized sizes (KB/MB) and replaced "Varies with device" entries with mean sizes by category.
- Installs: Removed symbols (+, commas) and cleaned non-numeric entries for numeric conversion.
- Rating & Reviews: Imputed 1,463 missing ratings with category-wise means; converted reviews to integer format.
- Last Updated: Standardized date format for accuracy.
- Category & Genres: Retained Category while removing redundant Genres for clarity.
- Current Version: Excluded due to inconsistencies across records.
- Android Version: Removed two rows with 'NaN' values.

Additional Transformations:

- Review & Install Categories: Binned data for balanced representation across reviews and installs.
- Log Transformed Installs: Applied log transformation to normalize install data and reduce skewness.
- Update Category: Classified updates as "Old" or "Recent" to analyze the impact on performance metrics.
</div>


# Data Visualizations
<div style="text-align: justify;">
Key Insights: Highlight patterns, distributions, correlations, or anomalies in the dataset (e.g., visualizations like histograms, scatter plots, or correlation matrices).
</div>


# **SMART Question**
<div style="text-align: justify;">
Which are the top 5 app categories, as identified by classification models (logistic regression, SVM, XGBoost, KNN, and random forest), that significantly influenced app success (measured by installs) based on app data from 2010 to 2018, and how accurately can these models predict success trends within this time period?
</div>



# **Model Selection Process**

* Logistic Regression:
-> Why?
-> Advs
-> Explain Parameter Selection

* K-Nearest Neighbour:
-> Why?
-> Advs
-> Explain Parameter Selection

* Random Forest Classifier:
-> Why?
-> Advs
-> Explain Parameter Selection

* XG Boost:
-> Why?
-> Advs
-> Explain Parameter Selection


* Support Vector Machine:
-> Why?
-> Advs
-> Explain Parameter Selection




# **Model Evaluation**
<div style="text-align: justify;">
Performance Metrics: Include accuracy, ROC-AUC, sensitivity, specificity, and any other relevant metrics.
Confusion Matrix: Summarize performance using the confusion matrix.
OOB Error: Discuss the OOB error as an internal validation measure.
Training vs. Testing Accuracy: Analyze and compare performance on different datasets to check for overfitting.
</div>


# **Predictions and Interpretability**
<div style="text-align: justify;">
Prediction Use Cases: Examples of actionable insights from the model (e.g., predicting app install categories, identifying trends).
Feature Importance: Highlight the most significant predictors identified by the model.
</div>


# **Reliability and Limitations**
<div style="text-align: justify;">
Reliability of Results: Discuss how reliable the results are, based on metrics, cross-validation, and OOB error.
*Limitations:*
Bias in the dataset.
Model interpretability limitations.
Potential overfitting.
Mitigation Strategies: Discuss how limitations were addressed (e.g., feature engineering, parameter tuning).
</div>


# **Future Work**
<div style="text-align: justify;">
Improvements: Suggest ways to improve the model, such as:
Incorporating additional data.
Trying advanced algorithms or ensembles.
Refining features or engineering new ones.
Further Analysis: Mention areas where additional analysis could add value (e.g., testing for seasonality, applying domain knowledge).
</div>


# **Conclusion**
<div style="text-align: justify;">
Summary of Findings: Summarize the outcomes and their implications.
Impact: Highlight the potential impact of your analysis and predictions.
</div>


# **References**
<div style="text-align: justify;">
Use APA style to cite all sources.
</div>