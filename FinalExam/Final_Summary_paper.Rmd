---
title: "Analysis On Google Play Store Apps"
author: Snehitha Tadapaneni, Sai Rachana Kandikattu, Amrutha Jayachandradhara, Wilona
  Nguyen, Pramod Krishnachari
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
      position: right
  pdf_document:
    toc: true
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, results = "markdown", message = F)
```

# **Introduction**

::: {style="text-align: justify;"}
In the fast-paced and competitive world of mobile applications,
understanding what makes an application successful is a pressing
challenge for developers, businesses, and researchers alike. With
Android dominating the mobile operating system landscape—powering over
2.5 billion active devices globally (Brandom, 2019)—the Google Play
Store has become the central hub for app distribution and user
engagement. Offering millions of applications across diverse categories,
the platform serves as an invaluable resource for analyzing user
behavior and preferences.

The key to an app's success lies in understanding the intricate
relationships between various features, such as app category, ratings,
reviews, content suitability, and technical compatibility. Identifying
patterns within these factors enables developers to design apps that
align with user expectations, thereby increasing downloads, engagement,
and retention rates. In this project, we leverage the comprehensive
dataset from the Google Play Store to predict app success based on these
features.

To address this problem, we have adopted a robust machine learning
approach, employing and comparing multiple classification models,
including Logistic Regression, k-Nearest Neighbors (kNN), Random Forest,
Support Vector Machines (SVM), and Extreme Gradient Boosting (XGBoost).
By systematically evaluating the performance of these models, we aim to
determine the most effective classifier for predicting app success.

This analysis not only provides actionable insights for app developers
and stakeholders but also contributes to the broader understanding of
user dynamics and preferences in the app ecosystem. Through this
research, we aim to bridge the gap between user expectations and app
design, empowering developers to make data-driven decisions that enhance
user satisfaction and engagement.
:::

# **Exploratory Data Analysis (EDA)**

### Dataset Overview

::: {style="text-align: justify;"}
The dataset used in this study is a popular collection of Google Play
Store apps, sourced from Kaggle (faisaljanjua0555, 2023). It contains
detailed information about 10,841 apps, organized across 13 variables,
with each row representing an individual app. The dataset covers a range
of app attributes, such as ratings, reviews, size, category, installs,
price, and content rating, providing valuable insights into the
characteristics that contribute to app popularity and user engagement.
This comprehensive data serves as a foundation for analyzing patterns in
user behavior, app performance, and trends within the mobile app
ecosystem.
:::

### Data Cleaning

::: {style="text-align: justify;"}
The Google Play Store dataset, as analyzed, contains missing values and
inconsistent formats across various columns. To clean and prepare this
dataset for analysis, several steps were taken to address these data
quality issues. Here is an overview of how missing records were managed
and further improvements that could be considered:

-   Duplicates: Removed 1,181 duplicate apps, reducing the dataset from
    10,841 to 9,660 unique apps to eliminate redundancy.
-   Price: Cleaned dollar symbols for numeric conversion and removed
    rows with missing or blank prices.
-   Type: Replaced one missing value with "Free" based on a logical
    inference.
-   Size: Standardized sizes (KB/MB) and replaced "Varies with device"
    entries with mean sizes by category.
-   Installs: Removed symbols (+, commas) and cleaned non-numeric
    entries for numeric conversion.
-   Rating & Reviews: Imputed 1,463 missing ratings with category-wise
    means; converted reviews to integer format.
-   Last Updated: Standardized date format for accuracy.
-   Category & Genres: Retained Category while removing redundant Genres
    for clarity.
-   Current Version: Excluded due to inconsistencies across records.
-   Android Version: Removed two rows with 'NaN' values.

Additional Transformations:

-   Review & Install Categories: Binned data for balanced representation
    across reviews and installs.
-   Log Transformed Installs: Applied log transformation to normalize
    install data and reduce skewness.
-   Update Category: Classified updates as "Old" or "Recent" to analyze
    the impact on performance metrics.
:::

# **Data Visualizations**

::: {style="text-align: justify;"}
**Figure 1**. Installs vs reviews
[description]

![](Inst_reviews.png){style="display: block; margin: 0 auto;"}


**Figure 1**. Installs vs ratings
[description]

![](Inst_rating.png){style="display: block; margin: 0 auto;"}



**Figure 1**. Installs vs Price

[description]
![](Inst_price.png){style="display: block; margin: 0 auto;"}



**Figure 1**. Installs vs Category
[description]

![](Inst_cat.png){style="display: block; margin: 0 auto;"}

# **Data Pre-processing**

::: {style="text-align: justify;"}
- *Category Encoding*<br>
The Category column was transformed into dummy variables using model.matrix(), and the original column was removed. Column names were standardized by replacing spaces with underscores.
- *Installs Classification*<br>
The Installs column was converted into a binary variable (Low Installs as 0 and High Installs as 1) based on the median value. A histogram was created to visualize the distribution of categories.
- *Last Updated Transformation*<br>
The Last.Updated column was converted into numeric values representing the days since the most recent update, ensuring a quantitative measure of recency.


-\>why choose installs as target variable????????

:::

# **SMART Question**

::: {style="text-align: justify;"}
Which are the top 5 app categories, as identified by classification
models (logistic regression, SVM, XGBoost, KNN, and random forest), that
significantly influenced app success (measured by installs) based on app
data from 2010 to 2018, and how accurately can these models predict
success trends within this time period?
:::

# **Model Selection Process**

## Logistic Regression

Logistic Regression model is considered because it is an interpretable
model that is commonly used for classification such as in this project,
the target varibale is binary (Low or High Installs). It estimates the
probability of an outcome, providing clear insights into the
relationship between predictors and the target variable through
interpretable coefficients. In addition, logistic regression performs
well with proper preprocessing and can handle multicollinearity and
feature selection effectively using regularization techniques. While it
may not be ideal for capturing complex, non-linear relationships,
logistic regression can be a strong baseline for this problem.

The initial logistic model includes all 39 variables and has the AIC
value of 2881.6. The stepwise selection is proceeded based on this
logistic model. The result shows that the stepwise model has AIC value
of 2850, indicating that this model performs better than the initial
model. The stepwise model includes 16 out of 39 variables: `Rating`,
`Reviews`, `Price`, `Content.Rating`, `Last.Updated`, `catBUSINESS`,
`catEDUCATION`, `catEVENTS`, `catFINANCE`, `catHOUSE_AND_HOME`,
`catMAPS_AND_NAVIGATION`, `catMEDICAL`, `catNEWS_AND_MAGAZINES` ,
`catPHOTOGRAPHY`, `catSPORTS`, `catVIDEO_PLAYERS`, `catPERSONALIZATION`,
`catBEAUTY`, `catPARENTING.`

In Figure 1., from this stepwise model, the top five importance features
from this model is: `Review`, `Price`, `Rating`, `Last.Updated`, and
`Content.Rating`. The top 5 categories that have most impacted in this
model is `FINANCE`, `PHOTOGRAPHY`, `EDUCATION`, `MEDICAL` and
`HOUSE_AND_HOME`.

::: {style="text-align: center;"}
**Figure 1**. Top 10 Important Features
:::

![](images/clipboard-1184837473.png){style="display: block; margin: 0 auto;"}

## K-Nearest Neighbour
-\> Why? -\> Advs -\> Explain Parameter

```         
Selection
```

## Random Forest Classifier

The Random Forest classifier is considered because it is a robust and flexible model that performs well in classification tasks, such as in this project, where the target variable is binary (Low or High Installs). It combines multiple decision trees to produce a more accurate and stable prediction through an ensemble approach. Random Forest handles non-linear relationships effectively and is resistant to overfitting due to its random sampling and feature selection techniques. Additionally, it provides insights into feature importance, which can guide further analysis and feature selection.

The initial Random Forest model is built using all 39 variables, and its performance metrics, such as accuracy and F1-score, demonstrate its capability to classify effectively. Feature importance analysis highlights the top predictors contributing to the model's performance. These include Reviews, Price, Rating, Last.Updated, Size, cat_MEDICAL, cat_EVENTS, cat_ENTERTAINMENT, cat_EDUCATION, cat_BUSINESS, and cat_GAME.

The model benefits from hyperparameter tuning to optimize its performance, ensuring that it balances complexity and accuracy. Although Random Forest may lack the direct interpretability of logistic regression, its ability to model complex interactions and rank features based on importance makes it a strong candidate for this classification task. Due to the overfitting ability of the model, I have tuned the 'mtry' taking values as 2,4,6,8,10. The model's ability to overfit has reduced for the value 'mtry': 4.

In Figure 2., from the mean decrease accuracy values, the top five importance features
from this model is: 'Reviews', 'Price', 'Rating', 'Last.Updated', 'Size'. The top 5 categories that have most impacted in this
model is 'cat_MEDICAL', 'cat_EVENTS', 'cat_ENTERTAINMENT', 'cat_EDUCATION', 'cat_BUSINESS', and 'cat_GAME'.

::: {style="text-align: center;"}
**Figure 2**. Top 10 Important Features for Random Forest Classifier
:::

![](feature_importance_accuracy_large.png){style="display: block; margin: 0 auto;"}

## XG Boost

-\> Why? -\> Advs -\> Explain Parameter Selection

## Support Vector Machine

-\> Why? -\> Advs -\> Explain Parameter

```         
Selection
```

# **Model Evaluation**

::: {style="text-align: justify;"}
Performance Metrics: Include accuracy, ROC-AUC, sensitivity,
specificity, and any other relevant metrics. Confusion Matrix: Summarize
performance using the confusion matrix. OOB Error: Discuss the OOB error
as an internal validation measure. Training vs. Testing Accuracy:
Analyze and compare performance on different datasets to check for
overfitting.
:::

## Logistic Model

### Confusion Matrix

The confusion matrix shows that the model correctly classified 5763
instances of class 0 and 4580 instances of class 1, with 370 false
positives and 127 false negatives. The overall accuracy is 95.42%,
indicating a high proportion of correct predictions, with a 95%
confidence interval of (95.0%, 95.8%). The model achieves a Kappa
statistic of 0.9072, reflecting strong agreement beyond chance.
Sensitivity (recall for class 0) is 97.84%, and specificity (ability to
identify class 1) is 92.53%, showing balanced performance. Positive
predictive value (precision for class 0) is 93.97%, and negative
predictive value (precision for class 1) is 97.30%. The balanced
accuracy, accounting for both sensitivity and specificity, is 95.18%.
The McNemar's test p-value (\<2.2e-16) suggests significant differences
between misclassifications, and the prevalence of class 0 is 54.34%.

::: {style="text-align: center;"}
**Figure 1**. Confusion Matrix of Logistic Regression
:::

![](images/clipboard-2963178691.png){style="display: block; margin: 0 auto;"}

### Precision, Racall Rate, F1-Score

The model achieves a precision of 93.97%, indicating that 93.97% of the
predictions for class 0 are correct. Its recall is 97.84%, meaning the
model successfully identifies 97.84% of all actual class 0 instances.
The F1 score, which balances precision and recall, is 95.87%,
highlighting the model's strong overall performance in accurately and
consistently classifying instances of class 0. This combination of high
precision and recall reflects the model's effectiveness in minimizing
both false positives and false negatives.

### ROC-AUC

The model achieves an AUC (Area Under the Curve) score of 0.9923,
indicating excellent performance in distinguishing between classes. This
demonstrates the model's strong predictive power and effectiveness in
separating class 0 and class 1 instances.

::: {style="text-align: center;"}
**Figure 1**. ROC-AUC Curve
:::

![](images/clipboard-2219338.png){style="display: block; margin: 0 auto;"}

## Random FOrest Classifier

### Confusion Matrix
The Random Forest model correctly classified 4425 instances of class 0 and 2918 instances of class 1, with 171 false positives and 202 false negatives. This results in an overall accuracy of 95.4%, indicating a high proportion of correct predictions. The performance is consistent across datasets, with a training accuracy of 96.3%, testing accuracy of 95.4%, and an Out-Of-Bag (OOB) accuracy of 95.2%, reflecting the model's robustness and minimal overfitting.

::: {style="text-align: center;"}
**Figure 3**. Confusion Matrix of Random Forest
:::

![](cm_RF.png){style="display: block; margin: 0 auto;"}

### Graph Analysis: Error Rates and Model Stability
The graph in the slide illustrates the error rates for the Random Forest model as the number of trees increases.

*Out-Of-Bag (OOB) Error:* The black line represents the OOB error rate, which stabilizes at a low level as the model grows more trees. This indicates that adding more trees contributes to better performance without overfitting.
*Class 1 and Class 2 Errors:* The red and green lines represent the error rates for class 1 and class 2, respectively. Both error rates decrease rapidly as the number of trees increases and stabilize, demonstrating the model's ability to effectively classify instances of both classes.
The error analysis shows that the model achieves convergence, with consistently low error rates across all classes, highlighting the reliability of the Random Forest classifier.
As, per the graph, the model is pretty reliable with generalizability.

::: {style="text-align: center;"}
**Figure 3**. Confusion Matrix of Random Forest
:::

![](RF_eval.png){style="display: block; margin: 0 auto;"}

# **Predictions and Interpretability**

::: {style="text-align: justify;"}
Prediction Use Cases: Examples of actionable insights from the model
(e.g., predicting app install categories, identifying trends). Feature
Importance: Highlight the most significant predictors identified by the
model.
:::

# **Reliability and Limitations**

::: {style="text-align: justify;"}
Reliability of Results: Discuss how reliable the results are, based on
metrics, cross-validation, and OOB error. *Limitations:* Bias in the
dataset. Model interpretability limitations. Potential overfitting.
Mitigation Strategies: Discuss how limitations were addressed (e.g.,
feature engineering, parameter tuning).
:::

# **Future Work**

::: {style="text-align: justify;"}
Improvements: Suggest ways to improve the model, such as: Incorporating
additional data. Trying advanced algorithms or ensembles. Refining
features or engineering new ones. Further Analysis: Mention areas where
additional analysis could add value (e.g., testing for seasonality,
applying domain knowledge).
:::

# **Conclusion**

::: {style="text-align: justify;"}
Summary of Findings: Summarize the outcomes and their implications.
Impact: Highlight the potential impact of your analysis and predictions.
:::

# **References**

::: {style="text-align: justify;"}
Use APA style to cite all sources.
:::
