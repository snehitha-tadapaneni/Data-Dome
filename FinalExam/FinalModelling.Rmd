---
title: "DATS 6101 - Analysis Google playstore apps "
date: "`r Sys.Date()`"
author: "Name: Snehitha Tadapaneni, Sai Rachana Kandikattu, Amrutha Jayachandradhara, Wilona Nguyen, Pramod Krishnachari
"
editor_options: 
  markdown: 
    wrap: 72
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, results = "markdown", message = F)
```

```{r chunk-label, options}
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course. 
# Importing the necessary libraries
library(ezids)
library(dplyr)
library(ggplot2)
library(DT)
library(corrplot)
library(lubridate)
library(tidyr)
library(scales)
library(cluster)
library(knitr)
library(kableExtra)
library(caret)
library(xgboost)
library(pROC)
library(e1071)
library(rpart)
library(rpart.plot)
library(randomForest)
library(viridis)
library(tidyverse)
library(recipes)
library(PRROC)
library(fmsb)
```

# Introduction

Our research focuses on exploring the featk

“Which are the top 5 app categories, as identified by classification models (logistic regression,SVM, XGBoost, KNN, and random forest), that significantly influenced app success (measured by installs) based on app data from 2010 to 2018, and how accurately can these models predict success trends within this time period??”


# Loading the Dataset

Here, we have loaded the dataset 'Google Play Store Apps' stored in csv file using ()

```{r}
#Loading the Dataset
data_apps <- data.frame(read.csv("googleplaystore.csv"))

```

#### Description of the App Dataset Columns
1) App: The name of the application, represented as a character string.
2) Category: The main category of the app, such as "ART_AND_DESIGN," represented as a character string.
3) Rating: The average user rating of the app, recorded as a numeric value.
4) Reviews: The total number of user reviews for the app, shown as a character string.
5) Size: The size of the application, represented as a character string.
6) Installs: The approximate number of installations for the app, stored as a character string.
7) Type: Indicates whether the app is free or paid, represented as a character string.
8) Price: The price of the app, stored as a character string. Free apps are listed as "0," while paid apps have a dollar amount.
9) Content.Rating: The target age group for the app, represented as a character string.
10) Genres: The genre(s) of the app.
11) Last.Updated: The date of the app's last update, stored as a character string.
12) Current.Ver: The current version of the app, represented as a character string.
13) Android.Ver: The minimum Android version required to run the app, stored as a character string.


# Summary of dataset after Data Cleaning
```{r}
#Dropping columns 
data_apps <- data_apps[, !(colnames(data_apps) %in% c("Android.Ver", "Current.Ver", "Type", "App", "Genres"))]

##Price----

### Convertion of Price to numerical 
data_apps$Price <- as.numeric(gsub("\\$", "", data_apps$Price)) #--Remove dollar symbols 
missing_na <- is.na(data_apps$Price)    
missing_blank <- data_apps$Price == "" 
data_apps <- data_apps[!is.na(data_apps$Price) & data_apps$Price != "", ] #-- Remove price is NA or blank

## Size---

#### Replacing Missing values with the mean (Size)
# Replace "Varies with Device" in the Size column with NA
data_apps$Size[data_apps$Size == "Varies with device"] <- NA #"Varies with Device" to NA
data_apps <- data_apps[!grepl("\\+", data_apps$Size), ]
data_apps$Size <- ifelse(grepl("k", data_apps$Size),
                          as.numeric(gsub("k", "", data_apps$Size)) *
0.001,  # Convert "K" to MB
                          as.numeric(gsub("M", "", data_apps$Size))) # Remove "M" for megabytes

# Calculate and display the mean size for each category in the 'Type' column
mean_size_by_type <- tapply(data_apps$Size, data_apps$Category,
mean, na.rm = TRUE)

# Loop through each row and replace NA values in the Size column with the mean size of the corresponding category
data_apps$Size <- ifelse(is.na(data_apps$Size),  # Check if Size is NA
  round(mean_size_by_type[data_apps$Category], 1), # Replace with the mean size based on the Category
  data_apps$Size)  # Keep the original size if it's not NA


##Installs---

####Remove the '+' sign, Remove the commas, Convert to numeric
#clean installations
clean_installs <- function(Installs) {
  Installs <- gsub("\\+", "", Installs)  
  Installs <- gsub(",", "", Installs)    
  return(as.numeric(Installs))           
}

data_apps$Installs <- sapply(data_apps$Installs, clean_installs)

nan_rows <- sapply(data_apps[, c("Size", "Installs")], function(x) any(is.nan(x)))

## Rating ---
data_apps <- data_apps %>%
  mutate(Rating = ifelse(is.na(Rating), mean(Rating, na.rm = TRUE), Rating))

# Identify the unique values in the 'Installs' column
unique_values <- unique(data_apps$Installs)

# Function to convert the installs to numeric
convert_to_numeric <- function(x) {
  # Remove non-numeric characters and convert to numeric
  as.numeric(gsub("[^0-9]", "", x)) * 10^(length(gregexpr(",", x)[[1]]) - 1)
}
# Sort unique values based on the custom numeric conversion
sorted_values <- unique_values[order(sapply(unique_values, convert_to_numeric))]

#Reviews---
data_apps$Reviews <- as.numeric(data_apps$Reviews)#Replace NA in Ratings with Overall Mean
data_apps <- data_apps %>%
  mutate(Rating = ifelse(is.na(Rating), mean(Rating, na.rm = TRUE), Rating))

#Content rating---
data_apps <- data_apps %>%
  mutate(
    Content.Rating = as.factor(Content.Rating)
  )
data_apps$Content.Rating <- as.numeric(data_apps$Content.Rating)

```

```{r}
#### Preprocessing for a model
#categories----
category_dummies <- model.matrix(~ Category - 1, data = data_apps)
colnames(category_dummies) <- gsub("Category", "cat", colnames(category_dummies))

# 3. Add dummy variables to the dataset and remove the original 'Category' column
data_apps <- cbind(data_apps, category_dummies)
data_apps$Category <- NULL

# 4. Replace spaces in column names with underscores
colnames(data_apps) <- gsub(" ", "_", colnames(data_apps))


#### Installs----
# Load necessary libraries

# Create two categories: Low Installs and High Installs
# Calculate the median of Installs to split into two categories
median_installs <- median(data_apps$Installs, na.rm = TRUE)


#Reclassify into two categories
data_apps$Installs_Category <- ifelse(data_apps$Installs <= median_installs, "Low Installs", "High Installs")


# Convert 'Installs_Category' to factor with levels "Low Installs" and "High Installs"
data_apps$Installs_Category <- factor(data_apps$Installs_Category,
                                         levels = c("Low Installs", "High Installs"),
                                         labels = c(0, 1))


# Check the conversion
table(data_apps$Installs_Category)

# Create a histogram for the new categories
ggplot(data_apps, aes(x = Installs_Category)) +
  geom_bar(stat = "count", fill = "skyblue", color = "black") +
  labs(title = "Histogram of Installs Category (Low vs High)",
       x = "Installs Category",
       y = "Count") +
  theme_minimal()


## Last updated----
# Convert the 'last_updated' column to Date type
data_apps$Last.Updated <- as.Date(data_apps$Last.Updated, format = "%B %d, %Y")

# Calculate the difference in days between the maximum date and each date in 'last_updated'
data_apps$Last.Updated <- as.numeric(difftime(max(data_apps$Last.Updated, na.rm = TRUE), 
                                       data_apps$Last.Updated, 
                                       units = "days"))
```

```{r}
# Display summary using kable
summary(data_apps) %>%
  kable(caption = "Summary of Google Play Store Apps") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE) %>%
  scroll_box(width = "100%", height = "400px")


```

## Logistic Regression
```{r}

```

## KNN 

### Model Training, Optimal k and Model Performance Evaluation
```{r}

# 1. Data Preparation for KNN
#---------------------------
# Select features
features <- c("Rating", "Reviews", "Size", "Price", "Content.Rating", "Last.Updated",
              grep("^cat", names(data_apps), value = TRUE))
X <- data_apps[, features]

# Convert target to factor with valid R variable names
y <- factor(data_apps$Installs_Category, 
            levels = c(0, 1),
            labels = c("low", "high"))  # Using 'low' and 'high' instead of 0/1

# Convert Last.Updated to proper date format
X$Last.Updated <- as.Date(X$Last.Updated)

# Split data with stratification
set.seed(123)
train_index <- createDataPartition(y, p = 0.7, list = FALSE, times = 1)
X_train <- X[train_index, ]
X_test <- X[-train_index, ]
y_train <- y[train_index]
y_test <- y[-train_index]

# Create preprocessing recipe
recipe_obj <- recipe(~ ., data = X_train) %>%
  # Extract date features after ensuring proper date format
  step_mutate(
    Year = year(Last.Updated),
    Month = month(Last.Updated),
    DayOfWeek = wday(Last.Updated)
  ) %>%
  # Remove original date column
  step_rm(Last.Updated) %>%
  # Log transform skewed features
  step_mutate(Reviews = log1p(Reviews)) %>%
  # Standardize numeric features
  step_normalize(all_numeric_predictors()) %>%
  # One-hot encode categorical variables
  step_dummy(all_nominal_predictors()) %>%
  # Remove highly correlated features
  step_corr(threshold = 0.9) %>%
  # Remove near-zero variance features
  step_nzv(freq_cut = 95/5)

# Prepare data using the recipe
prep_obj <- prep(recipe_obj, training = X_train)
X_train_processed <- bake(prep_obj, new_data = X_train)
X_test_processed <- bake(prep_obj, new_data = X_test)

# 2. Hyperparameter Tuning
#------------------------
# Define tuning grid
tuning_grid <- expand.grid(
  k = seq(1, 20, by = 2)
)

# Set up cross-validation
ctrl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 3,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = TRUE
)

# Train model with tuning
set.seed(123)
knn_tuned <- train(
  x = X_train_processed,
  y = y_train,
  method = "knn",
  tuneGrid = tuning_grid,
  trControl = ctrl,
  metric = "ROC",
  preProcess = NULL  # Already preprocessed
)

# Get optimal k
optimal_k <- knn_tuned$bestTune$k

# 3. Final Model Training
#-----------------------
final_model <- knn3(X_train_processed, y_train, k = optimal_k)

# Get predictions
pred_prob <- predict(final_model, X_test_processed, type = "prob")
pred_class <- factor(ifelse(pred_prob[,"high"] > 0.5, "high", "low"), 
                    levels = c("low", "high"))

# 4. Performance Evaluation
#------------------------
# ROC and AUC
roc_obj <- pROC::roc(as.numeric(y_test == "high"), pred_prob[,"high"])
auc_score <- as.numeric(pROC::auc(roc_obj))

# Confusion Matrix with optimal threshold
optimal_threshold <- coords(roc_obj, "best", ret = "threshold")$threshold
pred_class_optimal <- factor(ifelse(pred_prob[,"high"] > optimal_threshold, "high", "low"), 
                           levels = c("low", "high"))
conf_matrix <- confusionMatrix(pred_class_optimal, y_test)

# 5. Enhanced Visualizations
#-------------------------
# ROC Curve with confidence intervals
p1 <- ggroc(roc_obj) +
  geom_abline(linetype = "dashed") +
  annotate("text", x = 0.75, y = 0.25,
           label = sprintf("AUC = %.3f", auc_score)) +
  labs(title = "ROC Curve with Optimal Threshold",
       subtitle = sprintf("Optimal Threshold = %.2f", optimal_threshold)) +
  theme_minimal()

# Confusion Matrix Heatmap
conf_data <- as.data.frame(conf_matrix$table)
conf_data$Percentage <- conf_data$Freq / sum(conf_data$Freq) * 100

p2 <- ggplot(conf_data, aes(x = Reference, y = Prediction)) +
  geom_tile(aes(fill = Percentage), color = "white") +
  geom_text(aes(label = sprintf("%.1f%%\n(n=%d)", Percentage, Freq)), size = 4) +
  scale_fill_viridis() +
  labs(title = "Confusion Matrix",
       subtitle = sprintf("Accuracy: %.1f%% | Optimal k = %d", 
                         conf_matrix$overall["Accuracy"] * 100,
                         optimal_k)) +
  theme_minimal()

# Model Performance vs k
p3 <- ggplot(knn_tuned$results, aes(x = k)) +
  geom_line(aes(y = ROC, color = "ROC")) +
  geom_point(aes(y = ROC)) +
  geom_vline(xintercept = optimal_k, linetype = "dashed", color = "red") +
  labs(title = "Model Performance vs k",
       subtitle = paste("Optimal k =", optimal_k),
       x = "Number of Neighbors (k)",
       y = "ROC Score") +
  theme_minimal()

# Print results
print(p1)
print(p2)
print(p3)

cat("\nModel Performance Metrics:\n")
cat("------------------------\n")
print(conf_matrix)
cat("\nAUC Score:", round(auc_score, 3))
cat("\nOptimal k:", optimal_k)
cat("\nOptimal Threshold:", round(optimal_threshold, 3))


```

The code implements a K-Nearest Neighbors (KNN) classification model to predict app installation categories (high or low) based on several features like ratings, reviews, size, and more. It preprocesses the data through standardization, log transformations, date feature extraction, and encoding categorical variables. Hyperparameter tuning was performed to find the optimal number of neighbors (k = 11) using cross-validation, maximizing the ROC-AUC metric (0.962). The final model achieves an accuracy of 89.6%, with a balanced sensitivity (91.4%) and specificity (87.4%). The confusion matrix reveals some misclassification but demonstrates overall reliable performance. Optimal decision thresholds and visualization methods (ROC curve and confusion matrix) validate the model’s effectiveness.


### KNN Feature Importance and Visualizations for Model Metrics
```{r}

# Calculating correlation with target
calc_correlation_importance <- function(X, y) {
  # Converting categorical target to numeric
  y_num <- as.numeric(y == "high")
  
  # Correlation for each feature
  correlations <- sapply(X, function(x) {
    if(is.numeric(x)) {
      abs(cor(x, y_num, use = "complete.obs"))
    } else {
      0  # For non-numeric columns
    }
  })
  
  # Importance dataframe
  importance_df <- data.frame(
    feature = names(correlations),
    importance = abs(correlations)
  )
  
  # Sort by importance
  importance_df <- importance_df[order(-importance_df$importance), ]
  
  # Categorize importance
  importance_df$category <- ifelse(importance_df$importance >= median(importance_df$importance),
                                 "High", "Medium")
  
  return(importance_df)
}

# Calculate correlation-based importance
corr_importance <- calc_correlation_importance(X_train_processed, y_train)

# Create correlation-based feature importance plot
p4 <- ggplot(head(corr_importance, 15), 
       aes(x = reorder(feature, importance), 
           y = importance,
           fill = category)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_fill_manual(values = c("High" = "#4682B4", "Medium" = "#87CEEB")) +
  labs(title = "Top 15 Feature Importance Analysis",
       subtitle = "Based on Absolute Correlation with Target",
       x = "Feature",
       y = "Importance Score") +
  theme_minimal() +
  theme(legend.title = element_text(size = 10),
        legend.position = "top")

# Print top 15 most important features based on correlation
cat("\nTop 15 Most Important Features (Correlation-based):\n")
cat("-------------------------------------------\n")
print(head(corr_importance, 15))

# Calculate precision-recall curve
pr_obj <- pr.curve(scores.class0 = pred_prob[,"high"], 
                  weights.class0 = as.numeric(y_test == "high"),
                  curve = TRUE)

# Plot precision-recall curve using ggplot2
pr_data <- data.frame(
  recall = pr_obj$curve[,1],
  precision = pr_obj$curve[,2]
)

p5 <- ggplot(pr_data, aes(x = recall, y = precision)) +
  geom_line() +
  geom_area(alpha = 0.2) +
  labs(title = "Precision-Recall Curve",
       subtitle = sprintf("Average Precision = %.3f", pr_obj$auc.integral)) +
  theme_minimal()

# Probability distributions
prob_dist_data <- data.frame(
  probability = pred_prob[,"high"],
  actual_class = y_test
)

p6 <- ggplot(prob_dist_data, aes(x = probability, fill = actual_class)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = optimal_threshold, linetype = "dashed", color = "red") +
  labs(title = "Prediction Probability Distribution",
       subtitle = "By Actual Class with Optimal Threshold",
       x = "Predicted Probability (High Class)",
       y = "Density") +
  theme_minimal()

# Precision and Recall metrics
precision <- conf_matrix$byClass["Pos Pred Value"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- 2 * (precision * recall) / (precision + recall)

additional_metrics <- data.frame(
  Metric = c("F1 Score", 
             "Balanced Accuracy",
             "Precision",
             "Recall",
             "ROC AUC",
             "Log Loss"),
  Value = c(
    # F1 Score
    f1_score,
    # Balanced Accuracy
    conf_matrix$byClass["Balanced Accuracy"],
    # Precision
    precision,
    # Recall
    recall,
    # ROC AUC
    auc_score,
    # Log Loss
    mean(-log(ifelse(y_test == "high",
                     pred_prob[,"high"],
                     1 - pred_prob[,"high"])))
  )
)

# Printing plots
print(p4)
print(p5)
print(p6)

cat("\nAdditional Performance Metrics:\n")
cat("-----------------------------\n")
print(additional_metrics, row.names = FALSE)


# Prepare data for radar plot
metrics_radar <- data.frame(
  Accuracy = conf_matrix$overall["Accuracy"],
  AUC = auc_score,
  F1 = 2 * (precision * recall) / (precision + recall),
  Precision = precision,
  Recall = recall
)

# Add max and min rows required for radar plot
metrics_radar <- rbind(rep(1, ncol(metrics_radar)),  # Max values
                      rep(0, ncol(metrics_radar)),   # Min values
                      metrics_radar)

# Create radar plot
par(mar = c(1, 1, 4, 1))  # Adjust margins
radar_plot <- radarchart(metrics_radar,
                        axistype = 1,
                        pcol = "royalblue",
                        pfcol = rgb(0.4, 0.6, 0.9, 0.3),
                        plwd = 2,
                        cglcol = "grey",
                        cglty = 1,
                        cglwd = 0.8,
                        title = "Model Performance Metrics Overview")

# Create prediction confidence distribution plot
# Calculate confidence scores (max probability for each prediction)
confidence_scores <- apply(pred_prob, 1, max)

# Create confidence distribution plot using ggplot2
confidence_plot <- ggplot() +
  geom_density(aes(x = confidence_scores), color = "darkblue") +
  geom_histogram(aes(x = confidence_scores, y = ..density..), 
                bins = 30, 
                fill = "lightblue", 
                alpha = 0.5) +
  labs(title = "Distribution of Prediction Confidence",
       subtitle = "Higher values indicate more confident predictions",
       x = "Confidence Score",
       y = "Density") +
  theme_minimal() +
  xlim(0.5, 1.0) +  # Focus on the meaningful range
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

# Display plots
print(confidence_plot)

```
The code performs feature importance analysis and evaluates the KNN model’s performance using additional metrics and visualizations. The top features influencing the target variable (`Installs_Category`) include `Reviews`, `Month`, `Size`, `Year`, and category indicators such as `catGAME` and `catMEDICAL`. These features demonstrate strong correlations with the target, with `Reviews` being the most impactful.

Performance metrics such as an F1 Score (0.905), Precision (0.896), Recall (0.914), and AUC (0.962) indicate the model’s strong predictive capability. Visualizations like the precision-recall curve and radar chart highlight balanced performance, while the confidence distribution plot demonstrates reliable predictions with most probabilities near 1. The optimal threshold for classification further enhances decision-making accuracy.

#------------------------------------------------------------------------- 

## Decision Tree

#### Splitting the data into train and test split

```{r}
# Remove the Installs and Installs numerical columns
data <- data_apps[, !colnames(data_apps) %in% c("Installs")]

# Split the data into training and testing sets
set.seed(123)  # Ensure reproducibility
trainIndex <- createDataPartition(data$Installs_Category, p = 0.8, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
```

```{r}
# Fit the decision tree model
set.seed(42)
tree_model <- rpart(
  Installs_Category ~ . ,
  data = trainData,
  method = "class"
)

# Plot the decision tree
rpart.plot(tree_model, main = "Decision Tree for Predicting Installs Category")

```

```{r}
# Predict on training and test datasets
train_predictions <- predict(tree_model, trainData, type = "class")
test_predictions <- predict(tree_model, testData, type = "class")



# Calculate accuracy
train_accuracy <- sum(train_predictions == trainData$Installs_Category) / nrow(trainData)
test_accuracy <- sum(test_predictions == testData$Installs_Category) / nrow(testData)

# Print accuracy results
cat("Training Accuracy: ", train_accuracy, "\n")
cat("Test Accuracy: ", test_accuracy, "\n")



```

Why shift to Random Forest?
High Dimensionality: With 41 variables, random forest handles many features better and can identify the most important ones.<br>
Feature Importance: Random forest provides a ranking of feature importance, helping us understand which variables influence the Installs_Category.<br>
Accuracy: Random forest generally has better predictive accuracy for larger and more complex datasets.

## Random Forest

In this analysis, we employ a Random Forest model to predict the number of installs based on the top 5 app categories. The Random Forest algorithm is a robust ensemble learning method that builds multiple decision trees and combines their predictions to improve accuracy and reduce overfitting.

#### Building Random FOrest Classifier

```{r}
# Train the random forest model
set.seed(123)
rf_model <- randomForest(Installs_Category ~ .,
                         data = trainData,
                         ntree = 500,       # Number of trees
                         mtry = 4,  # Number of predictors sampled at each split
                         importance = TRUE) # Enable importance calculation

# Print the model summary
print(rf_model)
```
#### Plotting the Random Forest Result

```{r}
# Plot the Random Forest model
plot(rf_model, main = "Random Forest Model Performance")

# Add a legend to explain the colors
legend("topright",                              
       legend = c("OOB Error", "Class 1 Error", "Class 2 Error"), 
       col = c("black", "red", "green"),       
       lty = 1,                                
       cex = 0.8)   
```

#### Testing the accuracy

```{r}

# Predictions on the training set
train_predictions <- predict(rf_model, trainData)

# Predictions on the testing set
test_predictions <- predict(rf_model, testData)


# Confusion Matrix for Training Data
 train_cm <- confusionMatrix(train_predictions, trainData$Installs_Category)
 print(train_cm)

# Confusion Matrix for Testing Data
test_cm <- confusionMatrix(test_predictions, testData$Installs_Category)
print(test_cm)

```

#### Checking for training, testing and OOB datasets

```{r}
# Performance on Training Data
train_predictions <- predict(rf_model, newdata = trainData)
train_conf_matrix <- table(train_predictions, trainData$Installs_Category)
train_accuracy <- sum(diag(train_conf_matrix)) / sum(train_conf_matrix)
cat("Training Accuracy: ", train_accuracy, "\n")

# Performance on Testing Data
test_predictions <- predict(rf_model, newdata = testData)
test_conf_matrix <- table(test_predictions, testData$Installs_Category)
test_accuracy <- sum(diag(test_conf_matrix)) / sum(test_conf_matrix)
cat("Testing Accuracy: ", test_accuracy, "\n")

# OOB Error from the model
oob_error <- rf_model$err.rate[500, "OOB"]
oob_accuracy <- 1 - oob_error
cat("OOB Accuracy: ", oob_accuracy, "\n")

# Compare Results
comparison <- data.frame(
  Dataset = c("Training", "Testing", "OOB"),
  Accuracy = c(train_accuracy, test_accuracy, oob_accuracy)
)
print(comparison)

```

#### AUC/ROC 

```{r}
# Install and load pROC package if not already installed
if (!require("pROC")) install.packages("pROC", dependencies = TRUE)

# Get predicted probabilities for the positive class (class '1')
rf_prob <- predict(rf_model, testData, type = "prob")[, 2] # Probabilities for class '1'

# Compute ROC curve and AUC
roc_curve <- roc(testData$Installs_Category, rf_prob)

# Plot the ROC curve
plot(roc_curve, col = "blue", lwd = 2, main = "ROC Curve for Random Forest")
abline(a = 0, b = 1, lty = 2, col = "gray") # Diagonal reference line

# Display the AUC value
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")

```

#### Feature Importance Values

```{r}
# Variable importance
importance(rf_model)

# Plot variable importance
varImpPlot(rf_model)
```
#### Visualization for Feature Importance

```{r}
# Extract importance values
importance_values <- importance(rf_model)
importance_df <- data.frame(
  Feature = rownames(importance_values),
  MeanDecreaseAccuracy = importance_values[, "MeanDecreaseAccuracy"],
  MeanDecreaseGini = importance_values[, "MeanDecreaseGini"]
)

```

```{r}
# Plot Mean Decrease in Accuracy
accuracy_plot <- ggplot(importance_df, aes(x = reorder(Feature, MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(
    title = "Feature Importance (Mean Decrease in Accuracy)",
    x = "Features",
    y = "Importance"
  ) +
  theme_minimal() +
  theme(text = element_text(size = 12), axis.text.y = element_text(size = 10))

# Plot the accuracy plot
print(accuracy_plot)

# Save the plot with larger dimensions
# ggsave("feature_importance_accuracy_large.png", plot = accuracy_plot, width = 12, height = 10, dpi = 300)

# Plot Mean Decrease in Gini
gini_plot <- ggplot(importance_df, aes(x = reorder(Feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_bar(stat = "identity", fill = "lightgreen") +
  coord_flip() +
  labs(
    title = "Feature Importance (Mean Decrease in Gini)",
    x = "Features",
    y = "Importance"
  ) +
  theme_minimal() +
  theme(text = element_text(size = 12), axis.text.y = element_text(size = 10))

# Plot the gini index plot
print(gini_plot)

# Save the plot with larger dimensions
# ggsave("feature_importance_gini_large.png", plot = gini_plot, width = 12, height = 10, dpi = 300)


```

## Gradient Boosting

####  Data Splitting 
```{r}


# Separate features (X) and target (y)
#X <- data_final %>% select(-Installs, -Success)
X <- data_apps %>% select(-Installs, -Installs_Category)  # Exclude the target variable
y <- data_apps$Installs_Category             # Extract the target variable


table(y)
#Split into training and testing sets
set.seed(123)
train_index <- createDataPartition(y, p = 0.7, list = FALSE)

# Define X_train, X_test, y_train, y_test
X_train <- X[train_index, ] %>% mutate(across(everything(), as.numeric))
X_test <- X[-train_index, ] %>% mutate(across(everything(), as.numeric))
y_train <- as.numeric(as.character(y[train_index]))  # Convert to numeric
y_test <- as.numeric(as.character(y[-train_index]))  # Convert to numeric
```

```{r}

# Convert data to matrix for XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)

data_apps$Installs_Category <- factor(data_apps$Installs_Category, levels = c(0, 1))

```

#### Train Gradient Boosting Model
```{r}
# 4.  --------------------------------------

params <- list(
  objective = "binary:logistic",  # Binary classification
  eval_metric = "logloss",
  max_depth = 6,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Train the model
set.seed(42)
xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  watchlist = list(train = dtrain, test = dtest),
  early_stopping_rounds = 10,
  verbose = 1
)


```

#### Model Evaluation 

```{r}
# 

# Make predictions
y_pred <- predict(xgb_model, dtest)
y_pred_class <- ifelse(y_pred > 0.5, 1, 0)

# Confusion Matrix
y_pred_class <- factor(y_pred_class, levels = c(0, 1))
y_test <- factor(y_test, levels = c(0, 1))
conf_matrix <- confusionMatrix(y_pred_class, y_test)
print(conf_matrix)

```


```{r confusion-matrix-heatmap, echo=FALSE}
#Visulization of confusion matrix
# Code to generate and display the confusion matrix heatmap
cm_table <- as.data.frame(conf_matrix$table)
ggplot(cm_table, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "white", size = 5) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "Confusion Matrix", x = "Actual", y = "Predicted") +
  theme_minimal()
```

Confusion Matrix:The model struggles significantly with distinguishing between "Low Installs" and "High Installs," especially misclassifying many "Low Installs" as "High Installs" (1414 cases).
Precision for "High Installs" (proportion of correct "High Installs" predictions) and recall for "Low Installs" (proportion of correctly identified "Low Installs") are low.


```{r}
# AUC and ROC Curve
roc_obj <- roc(as.numeric(as.character(y_test)), y_pred)
auc_value <- auc(roc_obj)
cat("AUC:", auc_value, "\n")

# Plot ROC Curve
plot(roc_obj, main = "ROC Curve", col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")

```

The ROC curve illustrates the model's performance by plotting Sensitivity (True Positive Rate) against 1 - Specificity (False Positive Rate) across thresholds. The closer the curve approaches the top-left corner, the better the model distinguishes between "Low Installs" and "High Installs." A high AUC value indicates strong classification performance.

#### Feature Importance 

```{r}

importance_matrix <- xgb.importance(feature_names = colnames(X_train), model = xgb_model)
xgb.plot.importance(importance_matrix, top_n = 10, main = "Feature Importance")

# 7. Save Model ---------------------------------------------------------

xgb.save(xgb_model, "xgb_app_success.model")

# Summary
cat("Gradient Boosting achieved an accuracy of", conf_matrix$overall["Accuracy"],
    "and AUC of", auc_value, "\n")
```

"Reviews" has the highest influence, followed by "Rating" and "Last.Updated." Features like "Price" and "Size" also contribute, albeit minimally. Other categorical variables, such as "catFAMILY" and "catMEDICAL," have negligible impact on the model's predictions.

## SVM

```{r}
# Convert target variable to a factor
# Split the data into features and target
y <- as.factor(data_apps$Installs_Category)
X <- data_apps[, !names(data_apps) %in% c('Installs_Category', 'Installs')]

# Split data into training and testing sets
set.seed(42)
trainIndex <- createDataPartition(y, p = 0.75, list = FALSE)
X_train <- X[trainIndex, ]
X_test <- X[-trainIndex, ]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]

```



#### Checking if the boundary is non-linear or linear
```{r}
library(plotly)

# Prepare the plot data
plot_data <- data.frame(X_train, Class = as.factor(y_train))

# Create 3D scatter plot for features 1, 2, and 3
plot_1_2_3 <- plot_ly(data = plot_data, 
                      x = ~X_train[, 1], 
                      y = ~X_train[, 2], 
                      z = ~X_train[, 3], 
                      color = ~Class,
                      colors = c("red", "blue"),  # Set colors for classes (0 and 1)
                      type = 'scatter3d', 
                      mode = 'markers') %>%
  layout(title = "3D Scatter Plot: Feature 1 vs Feature 2 vs Feature 3",
         scene = list(xaxis = list(title = colnames(X_train)[1]),
                      yaxis = list(title = colnames(X_train)[2]),
                      zaxis = list(title = colnames(X_train)[3])))

# Create 3D scatter plot for features 4, 5, and 6
plot_4_5_6 <- plot_ly(data = plot_data, 
                      x = ~X_train[, 4], 
                      y = ~X_train[, 5], 
                      z = ~X_train[, 6], 
                      color = ~Class,
                      colors = c("red", "blue"),  # Set colors for classes (0 and 1)
                      type = 'scatter3d', 
                      mode = 'markers') %>%
  layout(title = "3D Scatter Plot: Feature 4 vs Feature 5 vs Feature 6",
         scene = list(xaxis = list(title = colnames(X_train)[4]),
                      yaxis = list(title = colnames(X_train)[5]),
                      zaxis = list(title = colnames(X_train)[6])))

# Show plots
plot_1_2_3
plot_4_5_6
```


As we can see we cannot decide if the boundary is linear or non-linear hence, lets make two models linear and non-linear SVM to check which one is a better fit.




#### Tuning to find the best parameter values for C and Gamma for SVM non-linear

```{r}


# Load necessary libraries
library(e1071)
library(caret)

# Assuming you have already defined X_train, y_train, X_test, y_test

# Combine the training data into a data frame
train_data <- as.data.frame(cbind(X_train, y_train))

# Set up k-fold cross-validation
set.seed(42)
train_control <- trainControl(method = "cv", number = 5)

# Define the tuning grid for 'C' and 'sigma' (gamma)
tune_grid <- expand.grid(C = c( 0.1, 1, 10, 100),
                          sigma = c(0.5, 1))

# Train the SVM model using radial kernel with the tuning grid
svm_model <- train(y_train ~ ., data = train_data,
                   method = "svmRadial",
                   tuneGrid = tune_grid,
                   trControl = train_control,scaled = TRUE)

# Print the results of the tuning
print(svm_model)

# Best model parameters
best_params <- svm_model$bestTune
cat("Best Parameters:\n")
print(best_params)
```

As seen for the training set the best accuracy is achieved when C = 100 and gamma is 0.5

#### Tuning to find the best parameter values for C  for SVM linear

```{r}


# Load necessary libraries
library(e1071)
library(caret)

# Assuming you have already defined X_train, y_train, X_test, y_test

# Combine the training data into a data frame
train_data <- as.data.frame(cbind(X_train, y_train))

# Set up k-fold cross-validation
set.seed(42)
train_control <- trainControl(method = "cv", number = 5)

# Define the tuning grid for 'C' and 'sigma' (gamma)
tune_grid <- expand.grid(C = c( 0.1, 1, 10, 100, 1000))

# Train the SVM model using radial kernel with the tuning grid
svm_model <- train(y_train ~ ., data = train_data,
                   method = "svmLinear",
                   tuneGrid = tune_grid,
                   trControl = train_control,scaled = TRUE)

# Print the results of the tuning
print(svm_model)

# Best model parameters
best_params <- svm_model$bestTune
cat("Best Parameters:\n")
print(best_params)
```

For linear model it could be seen that at C = 100, we attain an accuracy of 94 percent which suggests that its better to assume that the y was linearly seperable. 
Hence, now lets find the accuracy, ROC, AUC score of the test data.

#### Bias variance MSE caluclation

```{r}
# Load necessary libraries
library(e1071)
library(ggplot2)
library(reshape2)

# Assuming you have already defined X_train, y_train, X_test, y_test

# Combine training and test sets into data frames
train_data <- as.data.frame(cbind(X_train, y_train))
test_data <- as.data.frame(X_test)

# Initialize vectors to store metrics
C_values <- c(0.1, 1, 10, 100, 1000, 100000, 500000)
bias_values <- c()
variance_values <- c()
mse_values <- c()

# Loop through different values of C
for (C in C_values) {
  # Train the SVM model
  svm_model <- svm(y_train ~ ., data = train_data, kernel = "linear", cost = C, scale = TRUE)
  
  # Predict on the test set
  predictions <- predict(svm_model, newdata = test_data)
  
  # Convert predictions and actual test values to numeric for calculations
  predictions_numeric <- as.numeric(as.character(predictions))
  y_test_numeric <- as.numeric(as.character(y_test))
  
  # Calculate Bias: Mean squared difference between the true mean and the predicted mean
  mean_true <- mean(y_test_numeric)
  mean_pred <- mean(predictions_numeric)
  bias <- (mean_true - mean_pred)^2
  bias_values <- c(bias_values, bias)
  
  # Calculate Variance: Variability of the predictions
  variance <- var(predictions_numeric)
  variance_values <- c(variance_values, variance)
  
  # Calculate Mean Squared Error (MSE): Average squared error
  mse <- mean((y_test_numeric - predictions_numeric)^2)
  mse_values <- c(mse_values, mse)
}

# Combine results into a data frame for plotting
results <- data.frame(
  C = C_values,
  Bias = bias_values,
  Variance = variance_values,
  MSE = mse_values
)

# Reshape data for ggplot
results_melted <- melt(results, id.vars = "C", variable.name = "Metric", value.name = "Value")

# Plotting
ggplot(results_melted, aes(x = factor(C), y = Value, color = Metric, group = Metric)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  labs(title = "Bias-Variance Tradeoff and MSE for Different C Values",
       x = "C Values",
       y = "Value",
       color = "Metric") +
  theme_minimal(base_size = 15) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "top"
  ) +
  scale_color_manual(values = c("Bias" = "blue", "Variance" = "red", "MSE" = "green")) +
  scale_x_discrete(labels = function(x) paste0("C = ", x)) +
  geom_text(aes(label = round(Value, 2)), vjust = -0.5)  # Add value labels above points


```
The analysis of bias, variance, and mean squared error (MSE) across different values of C shows:
MSE dip: At C = 1000, the MSE is the minimum.
Low Bias and Variance: The variance seems to be little higher which is expected for large C but it is not much high compared to 10, 100.
 
Hence, better to select C = 1000 for the SVM model to achieve an optimal balance between bias, variance, and MSE.


#### Model Evalution for Test Data

```{r}
# Load necessary libraries
library(e1071)
library(pROC)  # For ROC and AUC

# Assuming you have already defined X_train, y_train, X_test, y_test

# Combine the training data into a data frame
train_data <- as.data.frame(cbind(X_train, y_train))

# Fit the SVM model with linear kernel
svm_model <- svm(y_train ~ ., data = train_data, kernel = "linear", cost = 1000, decision.values = TRUE,scaled = TRUE)

# Step 1: Make predictions on the test set
predictions <- predict(svm_model, newdata = as.data.frame(X_test))

# Step 2: Create confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = y_test)
cat("Confusion Matrix:\n")
print(confusion_matrix)

confusion_matrix_caret <- confusionMatrix(confusion_matrix)

# Step 2: Extract Precision, Recall, and F1 Score
precision <- confusion_matrix_caret$byClass['Precision']
recall <- confusion_matrix_caret$byClass['Recall']
f1_score <- confusion_matrix_caret$byClass['F1']

# Display the metrics
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1 Score:", f1_score, "\n")
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")

# Step 4: Get decision values for ROC curve
fitted <- attributes(predict(svm_model, newdata = as.data.frame(X_test), decision.values = TRUE))$decision.values

# Step 5: Generate ROC plot for the test set
roc_curve <- roc(y_test, -fitted)  # Note: Use negative for class labeling

# Plot the ROC curve
plot(roc_curve, main = "ROC Curve for Test Data")
# Add AUC to the plot
auc_value <- auc(roc_curve)
legend("bottomright", legend = paste("AUC =", round(auc_value, 2)), bty = "n")




```

The model achieved an impressive accuracy of 94% and a high AUC of 0.99, indicating excellent performance in classification. Additionally, the high precision, recall, and F1-score values reflect the model's capability to accurately classify both class 0 and class 1. These results suggest that the model is highly effective in distinguishing between the two classes, making it reliable for practical applications.



#### Feature importance for the model

```{r}
# Assuming you have already trained your SVM model (svm_model) using e1071

# Get coefficients from the SVM model
coefficients <- as.vector(svm_model$coefs) %*% svm_model$SV

# Get the intercept term
intercept <- svm_model$rho

# Combine coefficients and intercept into a single vector
all_coefficients <- c(intercept, coefficients)

# Print coefficients
cat("Coefficients (including intercept):\n")
print(all_coefficients)

# Check the number of coefficients
num_coefficients <- length(all_coefficients)
cat("Number of Coefficients (including intercept):", num_coefficients, "\n")

# Get feature names
feature_names <- colnames(X_train)

# Create a named vector for coefficients with feature names
named_coefficients <- setNames(coefficients, feature_names)

# Print named coefficients
cat("Feature Coefficients:\n")
print(named_coefficients)

# Sort coefficients by absolute value for feature importance
sorted_coefficients <- sort(abs(named_coefficients), decreasing = TRUE)

# Print sorted feature importance
cat("Sorted Feature Importance:\n")
print(sorted_coefficients)
top_coef = head(sorted_coefficients,15)
# Optional: Visualize feature importance
barplot(
  top_coef,
  main = "Feature Importance from SVM Coefficients",
  xlab = "Features",
  col = "steelblue",
  las = 2,
  cex.names = 0.3,# Adjust name size if necessary
  horiz = TRUE
)


```

The analysis shows that Reviews, Price, and Lastupdated are the top features influencing the model's performance. Additionally, the leading app categories—Finance, Medical, Tools, Events, and News and Mangazines —suggest that focusing on these areas can enhance the chances of app success.


