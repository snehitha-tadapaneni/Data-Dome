---
title: "DATS 6101 - Analysis Google playstore apps "
date: "`r Sys.Date()`"
author: "Name: Snehitha Tadapaneni, Sai Rachana Kandikattu, Amrutha Jayachandradhara, Wilona Nguyen, Pramod Krishnachari
"
editor_options: 
  markdown: 
    wrap: 72
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, results = "markdown", message = F)
```

```{r chunk-label, options}
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course. 
# Importing the necessary libraries
library(ezids)
library(dplyr)
library(ggplot2)
library(DT)
library(corrplot)
library(lubridate)
library(tidyr)
library(scales)
library(cluster)
library(knitr)
library(kableExtra)
library(caret)
library(xgboost)
library(pROC)
library(e1071)
library(rpart)
library(rpart.plot)
library(randomForest)
```

# Introduction

Our research focuses on exploring the featk

“Which are the top 5 app categories, as identified by classification
models (logistic regression,SVM, XGBoost, KNN, and random forest), that
significantly influenced app success (measured by installs) based on app
data from 2010 to 2018, and how accurately can these models predict
success trends within this time period?”

# Loading the Dataset

Here, we have loaded the dataset 'Google Play Store Apps' stored in csv
file using ()

```{r}
#Loading the Dataset
data_apps <- data.frame(read.csv("googleplaystore.csv"))

```

#### Description of the App Dataset Columns

1)  App: The name of the application, represented as a character string.
2)  Category: The main category of the app, such as "ART_AND_DESIGN,"
    represented as a character string.
3)  Rating: The average user rating of the app, recorded as a numeric
    value.
4)  Reviews: The total number of user reviews for the app, shown as a
    character string.
5)  Size: The size of the application, represented as a character
    string.
6)  Installs: The approximate number of installations for the app,
    stored as a character string.
7)  Type: Indicates whether the app is free or paid, represented as a
    character string.
8)  Price: The price of the app, stored as a character string. Free apps
    are listed as "0," while paid apps have a dollar amount.
9)  Content.Rating: The target age group for the app, represented as a
    character string.
10) Genres: The genre(s) of the app.
11) Last.Updated: The date of the app's last update, stored as a
    character string.
12) Current.Ver: The current version of the app, represented as a
    character string.
13) Android.Ver: The minimum Android version required to run the app,
    stored as a character string.

# Summary of dataset after Data Cleaning

```{r}
#Dropping columns 
data_apps <- data_apps[, !(colnames(data_apps) %in% c("Android.Ver", "Current.Ver", "Type", "App", "Genres"))]

##Price----

### Convertion of Price to numerical 
data_apps$Price <- as.numeric(gsub("\\$", "", data_apps$Price)) #--Remove dollar symbols 
missing_na <- is.na(data_apps$Price)    
missing_blank <- data_apps$Price == "" 
data_apps <- data_apps[!is.na(data_apps$Price) & data_apps$Price != "", ] #-- Remove price is NA or blank

## Size---

#### Replacing Missing values with the mean (Size)
# Replace "Varies with Device" in the Size column with NA
data_apps$Size[data_apps$Size == "Varies with device"] <- NA #"Varies with Device" to NA
data_apps <- data_apps[!grepl("\\+", data_apps$Size), ]
data_apps$Size <- ifelse(grepl("k", data_apps$Size),
                          as.numeric(gsub("k", "", data_apps$Size)) *
0.001,  # Convert "K" to MB
                          as.numeric(gsub("M", "", data_apps$Size))) # Remove "M" for megabytes

# Calculate and display the mean size for each category in the 'Type' column
mean_size_by_type <- tapply(data_apps$Size, data_apps$Category,
mean, na.rm = TRUE)

# Loop through each row and replace NA values in the Size column with the mean size of the corresponding category
data_apps$Size <- ifelse(is.na(data_apps$Size),  # Check if Size is NA
  round(mean_size_by_type[data_apps$Category], 1), # Replace with the mean size based on the Category
  data_apps$Size)  # Keep the original size if it's not NA


##Installs---

####Remove the '+' sign, Remove the commas, Convert to numeric
#clean installations
clean_installs <- function(Installs) {
  Installs <- gsub("\\+", "", Installs)  
  Installs <- gsub(",", "", Installs)    
  return(as.numeric(Installs))           
}

data_apps$Installs <- sapply(data_apps$Installs, clean_installs)

nan_rows <- sapply(data_apps[, c("Size", "Installs")], function(x) any(is.nan(x)))

## Rating ---
data_apps <- data_apps %>%
  mutate(Rating = ifelse(is.na(Rating), mean(Rating, na.rm = TRUE), Rating))

# Identify the unique values in the 'Installs' column
unique_values <- unique(data_apps$Installs)

# Function to convert the installs to numeric
convert_to_numeric <- function(x) {
  # Remove non-numeric characters and convert to numeric
  as.numeric(gsub("[^0-9]", "", x)) * 10^(length(gregexpr(",", x)[[1]]) - 1)
}
# Sort unique values based on the custom numeric conversion
sorted_values <- unique_values[order(sapply(unique_values, convert_to_numeric))]

#Reviews---
data_apps$Reviews <- as.numeric(data_apps$Reviews)#Replace NA in Ratings with Overall Mean
data_apps <- data_apps %>%
  mutate(Rating = ifelse(is.na(Rating), mean(Rating, na.rm = TRUE), Rating))

#Content rating---
data_apps <- data_apps %>%
  mutate(
    Content.Rating = as.factor(Content.Rating)
  )
data_apps$Content.Rating <- as.numeric(data_apps$Content.Rating)

```

```{r}
#### Preprocessing for a model
#categories----
category_dummies <- model.matrix(~ Category - 1, data = data_apps)
colnames(category_dummies) <- gsub("Category", "cat", colnames(category_dummies))

# 3. Add dummy variables to the dataset and remove the original 'Category' column
data_apps <- cbind(data_apps, category_dummies)
data_apps$Category <- NULL

# 4. Replace spaces in column names with underscores
colnames(data_apps) <- gsub(" ", "_", colnames(data_apps))


#### Installs----
# Load necessary libraries

# Create two categories: Low Installs and High Installs
# Calculate the median of Installs to split into two categories
median_installs <- median(data_apps$Installs, na.rm = TRUE)


#Reclassify into two categories
data_apps$Installs_Category <- ifelse(data_apps$Installs <= median_installs, "Low Installs", "High Installs")


# Convert 'Installs_Category' to factor with levels "Low Installs" and "High Installs"
data_apps$Installs_Category <- factor(data_apps$Installs_Category,
                                         levels = c("Low Installs", "High Installs"),
                                         labels = c(0, 1))


# Check the conversion
table(data_apps$Installs_Category)

# Create a histogram for the new categories
ggplot(data_apps, aes(x = Installs_Category)) +
  geom_bar(stat = "count", fill = "skyblue", color = "black") +
  labs(title = "Histogram of Installs Category (Low vs High)",
       x = "Installs Category",
       y = "Count") +
  theme_minimal()


## Last updated----
# Convert the 'last_updated' column to Date type
data_apps$Last.Updated <- as.Date(data_apps$Last.Updated, format = "%B %d, %Y")

# Calculate the difference in days between the maximum date and each date in 'last_updated'
data_apps$Last.Updated <- as.numeric(difftime(max(data_apps$Last.Updated, na.rm = TRUE), 
                                       data_apps$Last.Updated, 
                                       units = "days"))
```

```{r}
# Display summary using kable
summary(data_apps) %>%
  kable(caption = "Summary of Google Play Store Apps") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE) %>%
  scroll_box(width = "100%", height = "400px")


```

## Logistic Regression

```{r}
data_apps$Installs_Category <- as.factor(data_apps$Installs_Category)
data_apps <- data_apps[, !names(data_apps) %in% c('Installs')]
```

We first start with the basic Logistic model which includes at the
variables. The AIC value for this model is 2881.6.

```{r}
glm_model <- glm(Installs_Category ~ ., data = data_apps, family = binomial)
summary(glm_model)
```

### Stepwise Selection for Logistic Model

Next, the stepwise selection is proceeded based on the logistic model
above. It can be seen that the model below has AIC value of 2850,
indicating the below model performs better than the above (which has AIC
of 2881).

This model includes 16 features out of the 39 variables in the dataset.

```{r}
stepwise_model <- stats::step(glm_model, direction = "both", trace = 0)

summary(stepwise_model)
```

```{r}
glm_varImp <- varImp(stepwise_model, scale = FALSE)
print(glm_varImp)

```

### Feature Importance
```{r}
#install.packages('vip')
library(vip)


# Plot the top 10 important features with the custom color
vip_plot<-vip(stepwise_model, num_features = 10, bar = TRUE) 

vip_plot +
  # Set the custom color for bars
  geom_bar(stat = "identity", fill = "#365b6d")  +
  # Reversing the order of features for better visual flow
  coord_flip() +
  # Improving labels and text size
  theme_minimal() +
  theme(
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 16, face = "bold"),
    legend.position = "none"  # Hides the legend if not needed
  ) +
  ggtitle("Top 10 Important Features - GLM Model")


```

As it can be seen from the graph above, the top five importance features
from this model is: `Review`, `Price`, `Rating`, `Last.Updated`, and
`Content.Rating`. The top 5 categories that have most impacted in this
model is `FINANCE`, `PHOTOGRAPHY`, `EDUCATION`, `MEDICAL` and
`HOUSE_AND_HOME`.

### Model's Performance Evaluation

#### Confusion Matrix

The confusion matrix shows that the model correctly classified 5763
instances of class 0 and 4580 instances of class 1, with 370 false
positives and 127 false negatives. The overall accuracy is 95.42%,
indicating a high proportion of correct predictions, with a 95%
confidence interval of (95.0%, 95.8%). The model achieves a Kappa
statistic of 0.9072, reflecting strong agreement beyond chance.
Sensitivity (recall for class 0) is 97.84%, and specificity (ability to
identify class 1) is 92.53%, showing balanced performance. Positive
predictive value (precision for class 0) is 93.97%, and negative
predictive value (precision for class 1) is 97.30%. The balanced
accuracy, accounting for both sensitivity and specificity, is 95.18%.
The McNemar's test p-value (\<2.2e-16) suggests significant differences
between misclassifications, and the prevalence of class 0 is 54.34%.

```{r}
glm_probs <- predict(stepwise_model, data_apps, type = "response")

glm_pred <- ifelse(glm_probs > 0.5, 1, 0)

# Generate confusion matrix
test_cm <- confusionMatrix(factor(glm_pred), factor(data_apps$Installs_Category))
print(test_cm)

```

```{r}


# Extract confusion matrix table
cm_table <- as.table(test_cm)

# Convert the table to a data frame for ggplot
cm_df <- as.data.frame(cm_table)

cm_df <- as.data.frame(test_cm$table)
cm_df$Percentage <- cm_df$Freq / sum(cm_df$Freq) * 100

ggplot(cm_df, aes(x = Prediction, y = Reference)) +
  geom_tile(aes(fill = Percentage), color = "white") +
  geom_text(aes(label = sprintf("%.1f%%\n(n=%d)", Percentage, Freq)), 
            size = 4, color = "black") +
  scale_fill_gradient2(low = "white", high = "#6c9286", 
                      midpoint = min(cm_df$Percentage)) +
  labs(title = "Confusion Matrix Heatmap",
       subtitle = sprintf("Overall Accuracy: %.1f%%", test_cm$overall["Accuracy"] * 100)) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    axis.text = element_text(size = 10),
    legend.title = element_text(size = 10)
  )
```

#### Precision, Recal, F1-Score

The model achieves a precision of 93.97%, indicating that 93.97% of the
predictions for class 0 are correct. Its recall is 97.84%, meaning the
model successfully identifies 97.84% of all actual class 0 instances.
The F1 score, which balances precision and recall, is 95.87%,
highlighting the model's strong overall performance in accurately and
consistently classifying instances of class 0. This combination of high
precision and recall reflects the model's effectiveness in minimizing
both false positives and false negatives.

```{r}
precision <- test_cm$byClass["Pos Pred Value"]  # Precision
recall <- test_cm$byClass["Sensitivity"]        # Recall
f1_score <- test_cm$byClass["F1"]               # F1 Score

cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")
cat("F1 Score: ", f1_score, "\n")
```

#### ROC-AUC

The model achieves an AUC (Area Under the Curve) score of 0.9923,
indicating excellent performance in distinguishing between classes. This
demonstrates the model's strong predictive power and effectiveness in
separating class 0 and class 1 instances.

```{r}


# Predict probabilities
glm_probs <- predict(stepwise_model, data_apps, type = "response")

# Generate ROC curve
roc_curve <- roc(data_apps$Installs_Category, glm_probs)


# Plot ROC curve
plot(roc_curve, main = "ROC Curve")

# AUC (Area Under the Curve)
auc_value <- auc(roc_curve)
cat("AUC: ", auc_value, "\n")
```

## KNN

### Splitting the Data into Train and Test

```{r}
# Split the data into features and target
features <- c("Rating", "Reviews", "Size", "Price", "Content.Rating", "Last.Updated",
              grep("^cat", names(data_apps), value = TRUE))
X <- data_apps[, features]
y <- data_apps$Installs_Category

# Split the data into training and testing sets
set.seed(123)
train_index <- createDataPartition(y, p = 0.7, list = FALSE)
X_train <- X[train_index, ]
X_test <- X[-train_index, ]
y_train <- y[train_index]
y_test <- y[-train_index]

# Scale the features
preprocess_params <- preProcess(X_train, method = c("center", "scale"))
X_train_scaled <- predict(preprocess_params, X_train)
X_test_scaled <- predict(preprocess_params, X_test)
```

### Find optimal K using Cross validation

```{r}

# Train KNN model with cross-validation to find optimal k
k_values <- seq(1, 20, by = 2)
cv_results <- lapply(k_values, function(k) {
  train_control <- trainControl(method = "cv", number = 5)
  knn_model <- train(x = X_train_scaled, 
                    y = y_train,
                    method = "knn",
                    tuneGrid = data.frame(k = k),
                    trControl = train_control)
  return(knn_model$results$Accuracy)
})

# Plot k values vs accuracy
k_accuracy_df <- data.frame(k = k_values, accuracy = unlist(cv_results))
ggplot(k_accuracy_df, aes(x = k, y = accuracy)) +
  geom_line(color = "blue") +
  geom_point(color = "red") +
  labs(title = "K Values vs Cross-Validation Accuracy",
       x = "Number of Neighbors (k)",
       y = "Accuracy") +
  theme_minimal()
```

As we can see, the most optimal k is found by using CV and accuracy as
metric, highest accuracy of about 78% is achieved.

### Evaluation metrics for test data using optimal k

```{r}

optimal_k <- k_values[which.max(cv_results)]
final_knn <- knn3(X_train_scaled, y_train, k = optimal_k)

# Make predictions
y_pred_prob <- predict(final_knn, X_test_scaled, type = "prob")
y_pred <- factor(ifelse(y_pred_prob[,2] > 0.5, 1, 0), levels = c(0, 1))


y_test_numeric <- as.numeric(as.character(y_test))

# Calculate performance metrics
conf_matrix <- confusionMatrix(y_pred, y_test)
roc_curve <- roc(as.numeric(y_test_numeric), y_pred_prob[,2])
auc_score <- auc(roc_curve)

# Create performance summary table
performance_summary <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "F1 Score", "AUC"),
  Value = c(
    conf_matrix$overall["Accuracy"],
    conf_matrix$byClass["Precision"],
    conf_matrix$byClass["Recall"],
    conf_matrix$byClass["F1"],
    auc_score
  )
)
# Display performance summary using kable
kable(performance_summary, caption = "KNN Model Performance Summary") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE)

#Confusion matrix heat map
conf_matrix_df <- as.data.frame(conf_matrix$table)
conf_matrix_df$Percentage <- conf_matrix_df$Freq / sum(conf_matrix_df$Freq) * 100

ggplot(conf_matrix_df, aes(x = Prediction, y = Reference)) +
  geom_tile(aes(fill = Percentage), color = "white") +
  geom_text(aes(label = sprintf("%.1f%%\n(n=%d)", Percentage, Freq)), 
            size = 4, color = "black") +
  scale_fill_gradient2(low = "white", high = "#4A90E2", 
                      midpoint = min(conf_matrix_df$Percentage)) +
  labs(title = "Confusion Matrix Heatmap",
       subtitle = sprintf("Overall Accuracy: %.1f%%", conf_matrix$overall["Accuracy"] * 100)) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5),
    axis.text = element_text(size = 10),
    legend.title = element_text(size = 10)
  )

# Plot ROC curve
plot(roc_curve, main = "ROC Curve for KNN Model",
     col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")
legend("bottomright", legend = sprintf("AUC = %.3f", auc_score))
```

The AUC for the model is 0.83 which is good. But the performance metrics
such as accuracy, precision are anticipated to have an above 80% score.

# Feature importance

```{r}
calc_importance <- function(feature) {
  X_test_permuted <- X_test_scaled
  X_test_permuted[,feature] <- sample(X_test_scaled[,feature])
  pred_permuted <- predict(final_knn, X_test_permuted, type = "prob")
  roc_permuted <- roc(as.numeric(y_test), pred_permuted[,2])
  return(auc_score - auc(roc_permuted))
}

importance_scores <- sapply(features, calc_importance)
importance_df <- data.frame(
  Feature = features,
  Importance = importance_scores
)
importance_df <- importance_df[order(-importance_df$Importance),]

# Plot feature importance
ggplot(importance_df[1:39,], aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Feature Importance (KNN Model)",
       x = "Feature",
       y = "Importance Score") +
  theme_minimal()






```

The top 5 features contributing to the model are Size, Rating, Last
updated, Content Rating and Reviews.

#-------------------------------------------------------------------------

## Decision Tree

#### Splitting the data into train and test split

```{r}
# Remove the Installs and Installs numerical columns
data <- data_apps[, !colnames(data_apps) %in% c("Installs")]

# Split the data into training and testing sets
set.seed(123)  # Ensure reproducibility
trainIndex <- createDataPartition(data$Installs_Category, p = 0.8, list = FALSE)
trainData <- data[trainIndex, ]
testData<- data[-trainIndex, ]
```

```{r}
# Fit the decision tree model
set.seed(42)
tree_model <- rpart(
  Installs_Category ~ . ,
  data = trainData,
  method = "class"
)

# Plot the decision tree
rpart.plot(tree_model, main = "Decision Tree for Predicting Installs Category")

```

```{r}
# Predict on training and test datasets
train_predictions <- predict(tree_model, trainData, type = "class")
test_predictions <- predict(tree_model, testData, type = "class")



# Calculate accuracy
train_accuracy <- sum(train_predictions == trainData$Installs_Category) / nrow(trainData)
test_accuracy <- sum(test_predictions == testData$Installs_Category) / nrow(testData)

# Print accuracy results
cat("Training Accuracy: ", train_accuracy, "\n")
cat("Test Accuracy: ", test_accuracy, "\n")



```

Why shift to Random Forest? High Dimensionality: With 41 variables,
random forest handles many features better and can identify the most
important ones.<br> Feature Importance: Random forest provides a ranking
of feature importance, helping us understand which variables influence
the Installs_Category.<br> Accuracy: Random forest generally has better
predictive accuracy for larger and more complex datasets.

## Random Forest

In this analysis, we employ a Random Forest model to predict the number
of installs based on the top 5 app categories. The Random Forest
algorithm is a robust ensemble learning method that builds multiple
decision trees and combines their predictions to improve accuracy and
reduce overfitting.

#### Building Random FOrest Classifier

```{r}
# Train the random forest model
set.seed(123)
rf_model <- randomForest(Installs_Category ~ .,
                         data = trainData,
                         ntree = 500,       # Number of trees
                         mtry = 4,  # Number of predictors sampled at each split
                         importance = TRUE) # Enable importance calculation

# Print the model summary
print(rf_model)
```

#### Plotting the Random Forest Result

```{r}
# Plot the Random Forest model
plot(rf_model, main = "Random Forest Model Performance")

# Add a legend to explain the colors
legend("topright",                              
       legend = c("OOB Error", "Class 1 Error", "Class 2 Error"), 
       col = c("black", "red", "green"),       
       lty = 1,                                
       cex = 0.8)   
```

#### Testing the accuracy

```{r}

# Predictions on the training set
train_predictions <- predict(rf_model, trainData)

# Predictions on the testing set
test_predictions <- predict(rf_model, testData)


# Confusion Matrix for Training Data
 train_cm <- confusionMatrix(train_predictions, trainData$Installs_Category)
 print(train_cm)

# Confusion Matrix for Testing Data
test_cm <- confusionMatrix(test_predictions, testData$Installs_Category)
print(test_cm)

```

#### Checking for training, testing and OOB datasets

```{r}
# Performance on Training Data
train_predictions <- predict(rf_model, newdata = trainData)
train_conf_matrix <- table(train_predictions, trainData$Installs_Category)
train_accuracy <- sum(diag(train_conf_matrix)) / sum(train_conf_matrix)
cat("Training Accuracy: ", train_accuracy, "\n")

# Performance on Testing Data
test_predictions <- predict(rf_model, newdata = testData)
test_conf_matrix <- table(test_predictions, testData$Installs_Category)
test_accuracy <- sum(diag(test_conf_matrix)) / sum(test_conf_matrix)
cat("Testing Accuracy: ", test_accuracy, "\n")

# OOB Error from the model
oob_error <- rf_model$err.rate[500, "OOB"]
oob_accuracy <- 1 - oob_error
cat("OOB Accuracy: ", oob_accuracy, "\n")

# Compare Results
comparison <- data.frame(
  Dataset = c("Training", "Testing", "OOB"),
  Accuracy = c(train_accuracy, test_accuracy, oob_accuracy)
)
print(comparison)

```

#### AUC/ROC

```{r}
# Install and load pROC package if not already installed
if (!require("pROC")) install.packages("pROC", dependencies = TRUE)

# Get predicted probabilities for the positive class (class '1')
rf_prob <- predict(rf_model, testData, type = "prob")[, 2] # Probabilities for class '1'

# Compute ROC curve and AUC
roc_curve <- roc(testData$Installs_Category, rf_prob)

# Plot the ROC curve
plot(roc_curve, col = "blue", lwd = 2, main = "ROC Curve for Random Forest")
abline(a = 0, b = 1, lty = 2, col = "gray") # Diagonal reference line

# Display the AUC value
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")

```

#### Feature Importance Values

```{r}
# Variable importance
importance(rf_model)

# Plot variable importance
varImpPlot(rf_model)
```

#### Visualization for Feature Importance

```{r}
# Extract importance values
importance_values <- importance(rf_model)
importance_df <- data.frame(
  Feature = rownames(importance_values),
  MeanDecreaseAccuracy = importance_values[, "MeanDecreaseAccuracy"],
  MeanDecreaseGini = importance_values[, "MeanDecreaseGini"]
)

```

```{r}
# Plot Mean Decrease in Accuracy
accuracy_plot <- ggplot(importance_df, aes(x = reorder(Feature, MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(
    title = "Feature Importance (Mean Decrease in Accuracy)",
    x = "Features",
    y = "Importance"
  ) +
  theme_minimal() +
  theme(text = element_text(size = 12), axis.text.y = element_text(size = 10))

# Plot the accuracy plot
print(accuracy_plot)

# Save the plot with larger dimensions
# ggsave("feature_importance_accuracy_large.png", plot = accuracy_plot, width = 12, height = 10, dpi = 300)

# Plot Mean Decrease in Gini
gini_plot <- ggplot(importance_df, aes(x = reorder(Feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_bar(stat = "identity", fill = "lightgreen") +
  coord_flip() +
  labs(
    title = "Feature Importance (Mean Decrease in Gini)",
    x = "Features",
    y = "Importance"
  ) +
  theme_minimal() +
  theme(text = element_text(size = 12), axis.text.y = element_text(size = 10))

# Plot the gini index plot
print(gini_plot)

# Save the plot with larger dimensions
# ggsave("feature_importance_gini_large.png", plot = gini_plot, width = 12, height = 10, dpi = 300)


```

## Gradient Boosting

#### Data Splitting

```{r}


# Separate features (X) and target (y)
#X <- data_final %>% select(-Installs, -Success)
X <- data_apps %>% select(-Installs_Category)  # Exclude the target variable
y <- data_apps$Installs_Category             # Extract the target variable


table(y)
#Split into training and testing sets
set.seed(123)
train_index <- createDataPartition(y, p = 0.7, list = FALSE)

# Define X_train, X_test, y_train, y_test
X_train <- X[train_index, ] %>% mutate(across(everything(), as.numeric))
X_test <- X[-train_index, ] %>% mutate(across(everything(), as.numeric))
y_train <- as.numeric(as.character(y[train_index]))  # Convert to numeric
y_test <- as.numeric(as.character(y[-train_index]))  # Convert to numeric
```

```{r}

# Convert data to matrix for XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)

data_apps$Installs_Category <- factor(data_apps$Installs_Category, levels = c(0, 1))

```

#### Train Gradient Boosting Model

```{r}
# 4.  --------------------------------------

params <- list(
  objective = "binary:logistic",  # Binary classification
  eval_metric = "logloss",
  max_depth = 6,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Train the model
set.seed(42)
xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  watchlist = list(train = dtrain, test = dtest),
  early_stopping_rounds = 10,
  verbose = 1
)


```

#### Model Evaluation

```{r}
# 

# Make predictions
y_pred <- predict(xgb_model, dtest)
y_pred_class <- ifelse(y_pred > 0.5, 1, 0)

# Confusion Matrix
y_pred_class <- factor(y_pred_class, levels = c(0, 1))
y_test <- factor(y_test, levels = c(0, 1))
conf_matrix <- confusionMatrix(y_pred_class, y_test)
print(conf_matrix)

```

```{r confusion-matrix-heatmap, echo=FALSE}
#Visulization of confusion matrix
# Code to generate and display the confusion matrix heatmap
cm_table <- as.data.frame(conf_matrix$table)
ggplot(cm_table, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "white", size = 5) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "Confusion Matrix", x = "Actual", y = "Predicted") +
  theme_minimal()
```

Confusion Matrix:The model struggles significantly with distinguishing
between "Low Installs" and "High Installs," especially misclassifying
many "Low Installs" as "High Installs" (1414 cases). Precision for "High
Installs" (proportion of correct "High Installs" predictions) and recall
for "Low Installs" (proportion of correctly identified "Low Installs")
are low.

```{r}
# AUC and ROC Curve
roc_obj <- roc(as.numeric(as.character(y_test)), y_pred)
auc_value <- auc(roc_obj)
cat("AUC:", auc_value, "\n")

# Plot ROC Curve
plot(roc_obj, main = "ROC Curve", col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")

```

The ROC curve illustrates the model's performance by plotting
Sensitivity (True Positive Rate) against 1 - Specificity (False Positive
Rate) across thresholds. The closer the curve approaches the top-left
corner, the better the model distinguishes between "Low Installs" and
"High Installs." A high AUC value indicates strong classification
performance.

#### Feature Importance

```{r}

importance_matrix <- xgb.importance(feature_names = colnames(X_train), model = xgb_model)
xgb.plot.importance(importance_matrix, top_n = 10, main = "Feature Importance")

# 7. Save Model ---------------------------------------------------------

xgb.save(xgb_model, "xgb_app_success.model")

# Summary
cat("Gradient Boosting achieved an accuracy of", conf_matrix$overall["Accuracy"],
    "and AUC of", auc_value, "\n")
```

"Reviews" has the highest influence, followed by "Rating" and
"Last.Updated." Features like "Price" and "Size" also contribute, albeit
minimally. Other categorical variables, such as "catFAMILY" and
"catMEDICAL," have negligible impact on the model's predictions.

## SVM

```{r}
# Convert target variable to a factor
# Split the data into features and target
y <- as.factor(data_apps$Installs_Category)
X <- data_apps[, !names(data_apps) %in% c('Installs_Category')]

# Split data into training and testing sets
set.seed(42)
trainIndex <- createDataPartition(y, p = 0.75, list = FALSE)
X_train <- X[trainIndex, ]
X_test <- X[-trainIndex, ]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]

```

#### Checking if the boundary is non-linear or linear

```{r}
library(plotly)

# Prepare the plot data
plot_data <- data.frame(X_train, Class = as.factor(y_train))

# Create 3D scatter plot for features 1, 2, and 3
plot_1_2_3 <- plot_ly(data = plot_data, 
                      x = ~X_train[, 1], 
                      y = ~X_train[, 2], 
                      z = ~X_train[, 3], 
                      color = ~Class,
                      colors = c("red", "blue"),  # Set colors for classes (0 and 1)
                      type = 'scatter3d', 
                      mode = 'markers') %>%
  layout(title = "3D Scatter Plot: Feature 1 vs Feature 2 vs Feature 3",
         scene = list(xaxis = list(title = colnames(X_train)[1]),
                      yaxis = list(title = colnames(X_train)[2]),
                      zaxis = list(title = colnames(X_train)[3])))

# Create 3D scatter plot for features 4, 5, and 6
plot_4_5_6 <- plot_ly(data = plot_data, 
                      x = ~X_train[, 4], 
                      y = ~X_train[, 5], 
                      z = ~X_train[, 6], 
                      color = ~Class,
                      colors = c("red", "blue"),  # Set colors for classes (0 and 1)
                      type = 'scatter3d', 
                      mode = 'markers') %>%
  layout(title = "3D Scatter Plot: Feature 4 vs Feature 5 vs Feature 6",
         scene = list(xaxis = list(title = colnames(X_train)[4]),
                      yaxis = list(title = colnames(X_train)[5]),
                      zaxis = list(title = colnames(X_train)[6])))

# Show plots
plot_1_2_3
plot_4_5_6
```

As we can see we cannot decide if the boundary is linear or non-linear
hence, lets make two models linear and non-linear SVM to check which one
is a better fit.

#### Tuning to find the best parameter values for C and Gamma for SVM non-linear

```{r}


# Load necessary libraries
library(e1071)
library(caret)

# Assuming you have already defined X_train, y_train, X_test, y_test

# Combine the training data into a data frame
train_data <- as.data.frame(cbind(X_train, y_train))

# Set up k-fold cross-validation
set.seed(42)
train_control <- trainControl(method = "cv", number = 5)

# Define the tuning grid for 'C' and 'sigma' (gamma)
tune_grid <- expand.grid(C = c( 0.1, 1, 10, 100),
                          sigma = c(0.5, 1))

# Train the SVM model using radial kernel with the tuning grid
svm_model <- train(y_train ~ ., data = train_data,
                   method = "svmRadial",
                   tuneGrid = tune_grid,
                   trControl = train_control,scaled = TRUE)

# Print the results of the tuning
print(svm_model)

# Best model parameters
best_params <- svm_model$bestTune
cat("Best Parameters:\n")
print(best_params)
```

As seen for the training set the best accuracy is achieved when C = 100
and gamma is 0.5

#### Tuning to find the best parameter values for C for SVM linear

```{r}


# Load necessary libraries
library(e1071)
library(caret)

# Assuming you have already defined X_train, y_train, X_test, y_test

# Combine the training data into a data frame
train_data <- as.data.frame(cbind(X_train, y_train))

# Set up k-fold cross-validation
set.seed(42)
train_control <- trainControl(method = "cv", number = 5)

# Define the tuning grid for 'C' and 'sigma' (gamma)
tune_grid <- expand.grid(C = c( 0.1, 1, 10, 100, 1000))

# Train the SVM model using radial kernel with the tuning grid
svm_model <- train(y_train ~ ., data = train_data,
                   method = "svmLinear",
                   tuneGrid = tune_grid,
                   trControl = train_control,scaled = TRUE)

# Print the results of the tuning
print(svm_model)

# Best model parameters
best_params <- svm_model$bestTune
cat("Best Parameters:\n")
print(best_params)
```

For linear model it could be seen that at C = 100, we attain an accuracy
of 94 percent which suggests that its better to assume that the y was
linearly seperable. Hence, now lets find the accuracy, ROC, AUC score of
the test data.

#### Bias variance MSE caluclation

```{r}

# Load necessary libraries
library(e1071)
library(ggplot2)

# Assuming you have already defined X_train, y_train, X_test, y_test

# Combine training and test sets into data frames
train_data <- as.data.frame(cbind(X_train, y_train))
test_data <- as.data.frame(X_test)

# Initialize vectors to store MSE metrics
C_values <- c(0.1, 1, 10, 100, 1000, 100000, 500000)
mse_values <- c()

# Loop through different values of C
for (C in C_values) {
  # Train the SVM model
  svm_model <- svm(y_train ~ ., data = train_data, kernel = "linear", cost = C, scale = TRUE)
  
  # Predict on the test set
  predictions <- predict(svm_model, newdata = test_data)
  
  # Convert predictions and actual test values to numeric for MSE calculation
  predictions_numeric <- as.numeric(as.character(predictions))
  y_test_numeric <- as.numeric(as.character(y_test))
  
  # Calculate Mean Squared Error (MSE)
  mse <- mean((y_test_numeric - predictions_numeric)^2)
  mse_values <- c(mse_values, mse)
}

# Combine results into a data frame for plotting
results <- data.frame(
  C = C_values,
  MSE = mse_values
)

# Plotting MSE values
ggplot(results, aes(x = factor(C), y = MSE)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "red", size = 3) +
  labs(title = "MSE for Different C Values",
       x = "C Values",
       y = "Mean Squared Error (MSE)") +
  theme_minimal(base_size = 15) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold")
  ) +
  scale_x_discrete(labels = function(x) paste0("C = ", x)) +
  geom_text(aes(label = round(MSE, 2)), vjust = -0.5)  # Add value labels above points



```

The analysis of bias, variance, and mean squared error (MSE) across
different values of C shows: MSE dip: At C = 1000, the MSE is the
minimum. Low Bias and Variance: The variance seems to be little higher
which is expected for large C but it is not much high compared to 10,
100.

Hence, better to select C = 1000 for the SVM model to achieve an optimal
balance between bias, variance, and MSE.

#### Model Evalution for Test Data

```{r}
# Load necessary libraries
library(e1071)
library(pROC)  # For ROC and AUC

# Assuming you have already defined X_train, y_train, X_test, y_test

# Combine the training data into a data frame
train_data <- as.data.frame(cbind(X_train, y_train))

# Fit the SVM model with linear kernel
svm_model <- svm(y_train ~ ., data = train_data, kernel = "linear", cost = 1000, decision.values = TRUE,scaled = TRUE)

# Step 1: Make predictions on the test set
predictions <- predict(svm_model, newdata = as.data.frame(X_test))

# Step 2: Create confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = y_test)
cat("Confusion Matrix:\n")
print(confusion_matrix)

confusion_matrix_caret <- confusionMatrix(confusion_matrix)

# Step 2: Extract Precision, Recall, and F1 Score
precision <- confusion_matrix_caret$byClass['Precision']
recall <- confusion_matrix_caret$byClass['Recall']
f1_score <- confusion_matrix_caret$byClass['F1']

# Display the metrics
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1 Score:", f1_score, "\n")
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")

# Step 4: Get decision values for ROC curve
fitted <- attributes(predict(svm_model, newdata = as.data.frame(X_test), decision.values = TRUE))$decision.values

# Step 5: Generate ROC plot for the test set
roc_curve <- roc(y_test, -fitted)  # Note: Use negative for class labeling

# Plot the ROC curve
plot(roc_curve, main = "ROC Curve for Test Data")
# Add AUC to the plot
auc_value <- auc(roc_curve)
legend("bottomright", legend = paste("AUC =", round(auc_value, 2)), bty = "n")




```

The model achieved an impressive accuracy of 94% and a high AUC of 0.99,
indicating excellent performance in classification. Additionally, the
high precision, recall, and F1-score values reflect the model's
capability to accurately classify both class 0 and class 1. These
results suggest that the model is highly effective in distinguishing
between the two classes, making it reliable for practical applications.

#### Feature importance for the model

```{r}
# Assuming you have already trained your SVM model (svm_model) using e1071

# Get coefficients from the SVM model
coefficients <- as.vector(svm_model$coefs) %*% svm_model$SV

# Get the intercept term
intercept <- svm_model$rho

# Combine coefficients and intercept into a single vector
all_coefficients <- c(intercept, coefficients)

# Print coefficients
cat("Coefficients (including intercept):\n")
print(all_coefficients)

# Check the number of coefficients
num_coefficients <- length(all_coefficients)
cat("Number of Coefficients (including intercept):", num_coefficients, "\n")

# Get feature names
feature_names <- colnames(X_train)

# Create a named vector for coefficients with feature names
named_coefficients <- setNames(coefficients, feature_names)

# Print named coefficients
cat("Feature Coefficients:\n")
print(named_coefficients)

# Sort coefficients by absolute value for feature importance
sorted_coefficients <- sort(abs(named_coefficients), decreasing = TRUE)

# Print sorted feature importance
cat("Sorted Feature Importance:\n")
print(sorted_coefficients)
top_coef = head(sorted_coefficients,15)
# Optional: Visualize feature importance
barplot(
  top_coef,
  main = "Feature Importance from SVM Coefficients",
  xlab = "Features",
  col = "steelblue",
  las = 2,
  cex.names = 0.3,# Adjust name size if necessary
  horiz = TRUE
)


```

The analysis shows that Reviews, Price, and Lastupdated are the top
features influencing the model's performance. Additionally, the leading
app categories—Finance, Medical, Tools, Events, and News and Mangazines
—suggest that focusing on these areas can enhance the chances of app
success.
